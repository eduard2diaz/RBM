{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e69f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "with urllib.request.urlopen(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork/labs/Week4/data/utils.py\") as url:\n",
    "    response = url.read()\n",
    "target = open('utils.py', 'w')\n",
    "target.write(response.decode('utf-8'))\n",
    "target.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec2c883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# <span style=\"color:red\"><<<<<!!!!! ERROR !!!! please upgrade to TensorFlow 2.2.0, or restart your Kernel (Kernel->Restart & Clear Output)>>>>></span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
    "\n",
    "\n",
    "if not tf.__version__ == '2.2.0':\n",
    "    printmd('<<<<<!!!!! ERROR !!!! please upgrade to TensorFlow 2.2.0, or restart your Kernel (Kernel->Restart & Clear Output)>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04162b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from utils import tile_raster_images\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5bfd3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_bias = tf.Variable(tf.zeros([7]), tf.float32)\n",
    "h_bias = tf.Variable(tf.zeros([2]), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18d62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.constant(np.random.normal(loc=0.0, scale=1.0, size=(7, 2)).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "976217c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tf.Tensor([[1. 0. 0. 1. 0. 0. 0.]], shape=(1, 7), dtype=float32)\n",
      "hb:  tf.Tensor([0.1 0.1], shape=(2,), dtype=float32)\n",
      "w:  tf.Tensor(\n",
      "[[-0.36706835  1.1279109 ]\n",
      " [-0.26947287 -0.2715779 ]\n",
      " [ 0.03895997  0.79320604]\n",
      " [-1.0191058  -1.66996   ]\n",
      " [ 2.4419591   0.58952665]\n",
      " [ 0.8145571  -1.5592134 ]\n",
      " [-0.62143224  0.6927436 ]], shape=(7, 2), dtype=float32)\n",
      "p(h|v):  tf.Tensor([[0.21650107 0.3912528 ]], shape=(1, 2), dtype=float32)\n",
      "h0 states: tf.Tensor([[0. 0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant([[1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]], tf.float32)\n",
    "\n",
    "v_state = X\n",
    "print (\"Input: \", v_state)\n",
    "\n",
    "h_bias = tf.constant([0.1, 0.1])\n",
    "print (\"hb: \", h_bias)\n",
    "print (\"w: \", W)\n",
    "\n",
    "# Calculate the probabilities of turning the hidden units on:\n",
    "h_prob = tf.nn.sigmoid(tf.matmul(v_state, W) + h_bias)  #probabilities of the hidden units\n",
    "print (\"p(h|v): \", h_prob)\n",
    "\n",
    "# Draw samples from the distribution:\n",
    "h_state = tf.nn.relu(tf.sign(h_prob - tf.random.uniform(tf.shape(h_prob)))) #states\n",
    "print (\"h0 states:\", h_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3019911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:  tf.Tensor([0.1 0.2 0.1 0.1 0.1 0.2 0.1], shape=(7,), dtype=float32)\n",
      "p(vi∣h):  tf.Tensor([[0.5249792 0.549834  0.5249792 0.5249792 0.5249792 0.549834  0.5249792]], shape=(1, 7), dtype=float32)\n",
      "v probability states:  tf.Tensor([[1. 1. 1. 1. 1. 0. 1.]], shape=(1, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "vb = tf.constant([0.1, 0.2, 0.1, 0.1, 0.1, 0.2, 0.1])\n",
    "print (\"b: \", vb)\n",
    "v_prob = tf.nn.sigmoid(tf.matmul(h_state, tf.transpose(W)) + vb)\n",
    "print (\"p(vi∣h): \", v_prob)\n",
    "v_state = tf.nn.relu(tf.sign(v_prob - tf.random.uniform(tf.shape(v_prob))))\n",
    "print (\"v probability states: \", v_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2eb0200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input X: [[1. 0. 0. 1. 0. 0. 0.]]\n",
      "probablity vector: [0.5249792 0.549834  0.5249792 0.5249792 0.5249792 0.549834  0.5249792]\n",
      "probability of generating X:  0.005986424\n"
     ]
    }
   ],
   "source": [
    "inp = X\n",
    "print(\"input X:\" , inp.numpy())\n",
    "\n",
    "print(\"probablity vector:\" , v_prob[0].numpy())\n",
    "v_probability = 1\n",
    "\n",
    "for elm, p in zip(inp[0],v_prob[0]) :\n",
    "    if elm ==1:\n",
    "        v_probability *= p\n",
    "    else:\n",
    "        v_probability *= (1-p)\n",
    "\n",
    "print(\"probability of generating X: \" , v_probability.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50dffcd",
   "metadata": {},
   "source": [
    "# Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59edcab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading training and test data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(trX, trY), (teX, teY) = mnist.load_data()\n",
    "\n",
    "# showing an example of the Flatten class and operation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "flatten = Flatten(dtype='float32')\n",
    "trX = flatten(trX/255.0)\n",
    "trY = flatten(trY/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a035b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "vb = tf.Variable(tf.zeros([784]), tf.float32)\n",
    "hb = tf.Variable(tf.zeros([50]), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ac8d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,50]), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5249f2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v0_state = tf.Variable(tf.zeros([784]), tf.float32)\n",
    "\n",
    "#testing to see if the matrix product works\n",
    "tf.matmul( [v0_state], W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db4dc26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h0_state shape:  tf.Tensor([ 1 50], shape=(2,), dtype=int32)\n",
      "first 15 hidden states:  tf.Tensor([0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1.], shape=(15,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#computing the hidden nodes probability vector and checking shape\n",
    "h0_prob = tf.nn.sigmoid(tf.matmul([v0_state], W) + hb)  #probabilities of the hidden units\n",
    "print(\"h0_state shape: \" , tf.shape(h0_prob))\n",
    "\n",
    "#defining a function to return only the generated hidden states \n",
    "def hidden_layer(v0_state, W, hb):\n",
    "    h0_prob = tf.nn.sigmoid(tf.matmul([v0_state], W) + hb)  #probabilities of the hidden units\n",
    "    h0_state = tf.nn.relu(tf.sign(h0_prob - tf.random.uniform(tf.shape(h0_prob)))) #sample_h_given_X\n",
    "    return h0_state\n",
    "\n",
    "\n",
    "h0_state = hidden_layer(v0_state, W, hb)\n",
    "print(\"first 15 hidden states: \", h0_state[0][0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29df7ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden state shape:  (1, 50)\n",
      "v0 state shape:   (784,)\n",
      "v1 state shape:   (784,)\n"
     ]
    }
   ],
   "source": [
    "def reconstructed_output(h0_state, W, vb):\n",
    "    v1_prob = tf.nn.sigmoid(tf.matmul(h0_state, tf.transpose(W)) + vb) \n",
    "    v1_state = tf.nn.relu(tf.sign(v1_prob - tf.random.uniform(tf.shape(v1_prob)))) #sample_v_given_h\n",
    "    return v1_state[0]\n",
    "\n",
    "v1_state = reconstructed_output(h0_state, W, vb)\n",
    "print(\"hidden state shape: \", h0_state.shape)\n",
    "print(\"v0 state shape:  \", v0_state.shape)\n",
    "print(\"v1 state shape:  \", v1_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ce98f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0.47959185\n"
     ]
    }
   ],
   "source": [
    "def error(v0_state, v1_state):\n",
    "    return tf.reduce_mean(tf.square(v0_state - v1_state))\n",
    "\n",
    "err = tf.reduce_mean(tf.square(v0_state - v1_state))\n",
    "print(\"error\" , err.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c56ffca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_prob = tf.nn.sigmoid(tf.matmul([v1_state], W) + hb)\n",
    "h1_state = tf.nn.relu(tf.sign(h1_prob - tf.random.uniform(tf.shape(h1_prob)))) #sample_h_given_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74ab1a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  tf.Tensor(0.47959185, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"error: \", error(v0_state, v1_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017a942d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch #: 1  of 300 sample #: 199 reconstruction error: 0.168765\n",
      "Epoch: 0 batch #: 2  of 300 sample #: 199 reconstruction error: 0.224667\n",
      "Epoch: 0 batch #: 3  of 300 sample #: 199 reconstruction error: 0.111391\n",
      "Epoch: 0 batch #: 4  of 300 sample #: 199 reconstruction error: 0.121009\n",
      "Epoch: 0 batch #: 5  of 300 sample #: 199 reconstruction error: 0.113853\n",
      "Epoch: 0 batch #: 6  of 300 sample #: 199 reconstruction error: 0.176700\n",
      "Epoch: 0 batch #: 7  of 300 sample #: 199 reconstruction error: 0.084787\n",
      "Epoch: 0 batch #: 8  of 300 sample #: 199 reconstruction error: 0.106409\n",
      "Epoch: 0 batch #: 9  of 300 sample #: 199 reconstruction error: 0.104679\n",
      "Epoch: 0 batch #: 10  of 300 sample #: 199 reconstruction error: 0.190142\n",
      "Epoch: 0 batch #: 11  of 300 sample #: 199 reconstruction error: 0.094037\n",
      "Epoch: 0 batch #: 12  of 300 sample #: 199 reconstruction error: 0.141683\n",
      "Epoch: 0 batch #: 13  of 300 sample #: 199 reconstruction error: 0.067600\n",
      "Epoch: 0 batch #: 14  of 300 sample #: 199 reconstruction error: 0.108470\n",
      "Epoch: 0 batch #: 15  of 300 sample #: 199 reconstruction error: 0.118943\n",
      "Epoch: 0 batch #: 16  of 300 sample #: 199 reconstruction error: 0.106432\n",
      "Epoch: 0 batch #: 17  of 300 sample #: 199 reconstruction error: 0.075558\n",
      "Epoch: 0 batch #: 18  of 300 sample #: 199 reconstruction error: 0.123941\n",
      "Epoch: 0 batch #: 19  of 300 sample #: 199 reconstruction error: 0.140776\n",
      "Epoch: 0 batch #: 20  of 300 sample #: 199 reconstruction error: 0.109699\n",
      "Epoch: 0 batch #: 21  of 300 sample #: 199 reconstruction error: 0.097096\n",
      "Epoch: 0 batch #: 22  of 300 sample #: 199 reconstruction error: 0.074913\n",
      "Epoch: 0 batch #: 23  of 300 sample #: 199 reconstruction error: 0.101686\n",
      "Epoch: 0 batch #: 24  of 300 sample #: 199 reconstruction error: 0.134813\n",
      "Epoch: 0 batch #: 25  of 300 sample #: 199 reconstruction error: 0.096418\n",
      "Epoch: 0 batch #: 26  of 300 sample #: 199 reconstruction error: 0.100122\n",
      "Epoch: 0 batch #: 27  of 300 sample #: 199 reconstruction error: 0.102631\n",
      "Epoch: 0 batch #: 28  of 300 sample #: 199 reconstruction error: 0.081860\n",
      "Epoch: 0 batch #: 29  of 300 sample #: 199 reconstruction error: 0.093308\n",
      "Epoch: 0 batch #: 30  of 300 sample #: 199 reconstruction error: 0.082261\n",
      "Epoch: 0 batch #: 31  of 300 sample #: 199 reconstruction error: 0.089728\n",
      "Epoch: 0 batch #: 32  of 300 sample #: 199 reconstruction error: 0.108860\n",
      "Epoch: 0 batch #: 33  of 300 sample #: 199 reconstruction error: 0.053195\n",
      "Epoch: 0 batch #: 34  of 300 sample #: 199 reconstruction error: 0.075735\n",
      "Epoch: 0 batch #: 35  of 300 sample #: 199 reconstruction error: 0.066857\n",
      "Epoch: 0 batch #: 36  of 300 sample #: 199 reconstruction error: 0.081047\n",
      "Epoch: 0 batch #: 37  of 300 sample #: 199 reconstruction error: 0.057586\n",
      "Epoch: 0 batch #: 38  of 300 sample #: 199 reconstruction error: 0.068652\n",
      "Epoch: 0 batch #: 39  of 300 sample #: 199 reconstruction error: 0.102696\n",
      "Epoch: 0 batch #: 40  of 300 sample #: 199 reconstruction error: 0.111134\n",
      "Epoch: 0 batch #: 41  of 300 sample #: 199 reconstruction error: 0.093269\n",
      "Epoch: 0 batch #: 42  of 300 sample #: 199 reconstruction error: 0.074363\n",
      "Epoch: 0 batch #: 43  of 300 sample #: 199 reconstruction error: 0.117095\n",
      "Epoch: 0 batch #: 44  of 300 sample #: 199 reconstruction error: 0.069604\n",
      "Epoch: 0 batch #: 45  of 300 sample #: 199 reconstruction error: 0.064493\n",
      "Epoch: 0 batch #: 46  of 300 sample #: 199 reconstruction error: 0.032932\n",
      "Epoch: 0 batch #: 47  of 300 sample #: 199 reconstruction error: 0.029851\n",
      "Epoch: 0 batch #: 48  of 300 sample #: 199 reconstruction error: 0.044948\n",
      "Epoch: 0 batch #: 49  of 300 sample #: 199 reconstruction error: 0.108957\n",
      "Epoch: 0 batch #: 50  of 300 sample #: 199 reconstruction error: 0.048777\n",
      "Epoch: 0 batch #: 51  of 300 sample #: 199 reconstruction error: 0.063391\n",
      "Epoch: 0 batch #: 52  of 300 sample #: 199 reconstruction error: 0.032359\n",
      "Epoch: 0 batch #: 53  of 300 sample #: 199 reconstruction error: 0.105407\n",
      "Epoch: 0 batch #: 54  of 300 sample #: 199 reconstruction error: 0.041275\n",
      "Epoch: 0 batch #: 55  of 300 sample #: 199 reconstruction error: 0.100601\n",
      "Epoch: 0 batch #: 56  of 300 sample #: 199 reconstruction error: 0.059212\n",
      "Epoch: 0 batch #: 57  of 300 sample #: 199 reconstruction error: 0.050931\n",
      "Epoch: 0 batch #: 58  of 300 sample #: 199 reconstruction error: 0.064303\n",
      "Epoch: 0 batch #: 59  of 300 sample #: 199 reconstruction error: 0.068831\n",
      "Epoch: 0 batch #: 60  of 300 sample #: 199 reconstruction error: 0.111146\n",
      "Epoch: 0 batch #: 61  of 300 sample #: 199 reconstruction error: 0.060569\n",
      "Epoch: 0 batch #: 62  of 300 sample #: 199 reconstruction error: 0.078643\n",
      "Epoch: 0 batch #: 63  of 300 sample #: 199 reconstruction error: 0.053695\n",
      "Epoch: 0 batch #: 64  of 300 sample #: 199 reconstruction error: 0.084027\n",
      "Epoch: 0 batch #: 65  of 300 sample #: 199 reconstruction error: 0.100861\n",
      "Epoch: 0 batch #: 66  of 300 sample #: 199 reconstruction error: 0.066998\n",
      "Epoch: 0 batch #: 67  of 300 sample #: 199 reconstruction error: 0.049870\n",
      "Epoch: 0 batch #: 68  of 300 sample #: 199 reconstruction error: 0.063644\n",
      "Epoch: 0 batch #: 69  of 300 sample #: 199 reconstruction error: 0.075509\n",
      "Epoch: 0 batch #: 70  of 300 sample #: 199 reconstruction error: 0.075795\n",
      "Epoch: 0 batch #: 71  of 300 sample #: 199 reconstruction error: 0.076810\n",
      "Epoch: 0 batch #: 72  of 300 sample #: 199 reconstruction error: 0.054498\n",
      "Epoch: 0 batch #: 73  of 300 sample #: 199 reconstruction error: 0.088484\n",
      "Epoch: 0 batch #: 74  of 300 sample #: 199 reconstruction error: 0.104868\n",
      "Epoch: 0 batch #: 75  of 300 sample #: 199 reconstruction error: 0.056171\n",
      "Epoch: 0 batch #: 76  of 300 sample #: 199 reconstruction error: 0.102386\n",
      "Epoch: 0 batch #: 77  of 300 sample #: 199 reconstruction error: 0.117170\n",
      "Epoch: 0 batch #: 78  of 300 sample #: 199 reconstruction error: 0.110972\n",
      "Epoch: 0 batch #: 79  of 300 sample #: 199 reconstruction error: 0.107987\n",
      "Epoch: 0 batch #: 80  of 300 sample #: 199 reconstruction error: 0.030340\n",
      "Epoch: 0 batch #: 81  of 300 sample #: 199 reconstruction error: 0.057667\n",
      "Epoch: 0 batch #: 82  of 300 sample #: 199 reconstruction error: 0.047968\n",
      "Epoch: 0 batch #: 83  of 300 sample #: 199 reconstruction error: 0.081422\n",
      "Epoch: 0 batch #: 84  of 300 sample #: 199 reconstruction error: 0.080299\n",
      "Epoch: 0 batch #: 85  of 300 sample #: 199 reconstruction error: 0.052101\n",
      "Epoch: 0 batch #: 86  of 300 sample #: 199 reconstruction error: 0.092622\n",
      "Epoch: 0 batch #: 87  of 300 sample #: 199 reconstruction error: 0.073895\n",
      "Epoch: 0 batch #: 88  of 300 sample #: 199 reconstruction error: 0.037826\n",
      "Epoch: 0 batch #: 89  of 300 sample #: 199 reconstruction error: 0.052931\n",
      "Epoch: 0 batch #: 90  of 300 sample #: 199 reconstruction error: 0.080072\n",
      "Epoch: 0 batch #: 91  of 300 sample #: 199 reconstruction error: 0.053833\n",
      "Epoch: 0 batch #: 92  of 300 sample #: 199 reconstruction error: 0.066874\n",
      "Epoch: 0 batch #: 93  of 300 sample #: 199 reconstruction error: 0.106164\n",
      "Epoch: 0 batch #: 94  of 300 sample #: 199 reconstruction error: 0.110709\n",
      "Epoch: 0 batch #: 95  of 300 sample #: 199 reconstruction error: 0.052380\n",
      "Epoch: 0 batch #: 96  of 300 sample #: 199 reconstruction error: 0.051279\n",
      "Epoch: 0 batch #: 97  of 300 sample #: 199 reconstruction error: 0.115099\n",
      "Epoch: 0 batch #: 98  of 300 sample #: 199 reconstruction error: 0.091960\n",
      "Epoch: 0 batch #: 99  of 300 sample #: 199 reconstruction error: 0.057476\n",
      "Epoch: 0 batch #: 100  of 300 sample #: 199 reconstruction error: 0.110288\n",
      "Epoch: 0 batch #: 101  of 300 sample #: 199 reconstruction error: 0.068840\n",
      "Epoch: 0 batch #: 102  of 300 sample #: 199 reconstruction error: 0.083927\n",
      "Epoch: 0 batch #: 103  of 300 sample #: 199 reconstruction error: 0.078394\n",
      "Epoch: 0 batch #: 104  of 300 sample #: 199 reconstruction error: 0.060923\n",
      "Epoch: 0 batch #: 105  of 300 sample #: 199 reconstruction error: 0.060041\n",
      "Epoch: 0 batch #: 106  of 300 sample #: 199 reconstruction error: 0.112504\n",
      "Epoch: 0 batch #: 107  of 300 sample #: 199 reconstruction error: 0.135302\n",
      "Epoch: 0 batch #: 108  of 300 sample #: 199 reconstruction error: 0.062574\n",
      "Epoch: 0 batch #: 109  of 300 sample #: 199 reconstruction error: 0.116082\n",
      "Epoch: 0 batch #: 110  of 300 sample #: 199 reconstruction error: 0.057341\n",
      "Epoch: 0 batch #: 111  of 300 sample #: 199 reconstruction error: 0.103448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch #: 112  of 300 sample #: 199 reconstruction error: 0.050260\n",
      "Epoch: 0 batch #: 113  of 300 sample #: 199 reconstruction error: 0.078916\n",
      "Epoch: 0 batch #: 114  of 300 sample #: 199 reconstruction error: 0.060934\n",
      "Epoch: 0 batch #: 115  of 300 sample #: 199 reconstruction error: 0.057780\n",
      "Epoch: 0 batch #: 116  of 300 sample #: 199 reconstruction error: 0.052111\n",
      "Epoch: 0 batch #: 117  of 300 sample #: 199 reconstruction error: 0.049525\n",
      "Epoch: 0 batch #: 118  of 300 sample #: 199 reconstruction error: 0.079968\n",
      "Epoch: 0 batch #: 119  of 300 sample #: 199 reconstruction error: 0.111358\n",
      "Epoch: 0 batch #: 120  of 300 sample #: 199 reconstruction error: 0.075346\n",
      "Epoch: 0 batch #: 121  of 300 sample #: 199 reconstruction error: 0.037060\n",
      "Epoch: 0 batch #: 122  of 300 sample #: 199 reconstruction error: 0.096225\n",
      "Epoch: 0 batch #: 123  of 300 sample #: 199 reconstruction error: 0.072066\n",
      "Epoch: 0 batch #: 124  of 300 sample #: 199 reconstruction error: 0.071513\n",
      "Epoch: 0 batch #: 125  of 300 sample #: 199 reconstruction error: 0.050703\n",
      "Epoch: 0 batch #: 126  of 300 sample #: 199 reconstruction error: 0.080405\n",
      "Epoch: 0 batch #: 127  of 300 sample #: 199 reconstruction error: 0.082847\n",
      "Epoch: 0 batch #: 128  of 300 sample #: 199 reconstruction error: 0.068747\n",
      "Epoch: 0 batch #: 129  of 300 sample #: 199 reconstruction error: 0.058198\n",
      "Epoch: 0 batch #: 130  of 300 sample #: 199 reconstruction error: 0.070082\n",
      "Epoch: 0 batch #: 131  of 300 sample #: 199 reconstruction error: 0.074136\n",
      "Epoch: 0 batch #: 132  of 300 sample #: 199 reconstruction error: 0.046161\n",
      "Epoch: 0 batch #: 133  of 300 sample #: 199 reconstruction error: 0.053704\n",
      "Epoch: 0 batch #: 134  of 300 sample #: 199 reconstruction error: 0.113504\n",
      "Epoch: 0 batch #: 135  of 300 sample #: 199 reconstruction error: 0.089441\n",
      "Epoch: 0 batch #: 136  of 300 sample #: 199 reconstruction error: 0.109357\n",
      "Epoch: 0 batch #: 137  of 300 sample #: 199 reconstruction error: 0.062433\n",
      "Epoch: 0 batch #: 138  of 300 sample #: 199 reconstruction error: 0.069161\n",
      "Epoch: 0 batch #: 139  of 300 sample #: 199 reconstruction error: 0.108554\n",
      "Epoch: 0 batch #: 140  of 300 sample #: 199 reconstruction error: 0.073540\n",
      "Epoch: 0 batch #: 141  of 300 sample #: 199 reconstruction error: 0.090233\n",
      "Epoch: 0 batch #: 142  of 300 sample #: 199 reconstruction error: 0.062817\n",
      "Epoch: 0 batch #: 143  of 300 sample #: 199 reconstruction error: 0.054591\n",
      "Epoch: 0 batch #: 144  of 300 sample #: 199 reconstruction error: 0.074991\n",
      "Epoch: 0 batch #: 145  of 300 sample #: 199 reconstruction error: 0.082967\n",
      "Epoch: 0 batch #: 146  of 300 sample #: 199 reconstruction error: 0.103645\n",
      "Epoch: 0 batch #: 147  of 300 sample #: 199 reconstruction error: 0.066764\n",
      "Epoch: 0 batch #: 148  of 300 sample #: 199 reconstruction error: 0.075870\n",
      "Epoch: 0 batch #: 149  of 300 sample #: 199 reconstruction error: 0.039380\n",
      "Epoch: 0 batch #: 150  of 300 sample #: 199 reconstruction error: 0.046766\n",
      "Epoch: 0 batch #: 151  of 300 sample #: 199 reconstruction error: 0.108387\n",
      "Epoch: 0 batch #: 152  of 300 sample #: 199 reconstruction error: 0.084580\n",
      "Epoch: 0 batch #: 153  of 300 sample #: 199 reconstruction error: 0.061153\n",
      "Epoch: 0 batch #: 154  of 300 sample #: 199 reconstruction error: 0.084902\n",
      "Epoch: 0 batch #: 155  of 300 sample #: 199 reconstruction error: 0.073162\n",
      "Epoch: 0 batch #: 156  of 300 sample #: 199 reconstruction error: 0.067639\n",
      "Epoch: 0 batch #: 157  of 300 sample #: 199 reconstruction error: 0.075507\n",
      "Epoch: 0 batch #: 158  of 300 sample #: 199 reconstruction error: 0.062516\n",
      "Epoch: 0 batch #: 159  of 300 sample #: 199 reconstruction error: 0.059792\n",
      "Epoch: 0 batch #: 160  of 300 sample #: 199 reconstruction error: 0.068537\n",
      "Epoch: 0 batch #: 161  of 300 sample #: 199 reconstruction error: 0.081404\n",
      "Epoch: 0 batch #: 162  of 300 sample #: 199 reconstruction error: 0.118160\n",
      "Epoch: 0 batch #: 163  of 300 sample #: 199 reconstruction error: 0.033414\n",
      "Epoch: 0 batch #: 164  of 300 sample #: 199 reconstruction error: 0.057252\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "alpha = 0.01\n",
    "epochs = 1\n",
    "batchsize = 200\n",
    "weights = []\n",
    "errors = []\n",
    "batch_number = 0\n",
    "K = 1\n",
    "\n",
    "#creating datasets\n",
    "train_ds = \\\n",
    "    tf.data.Dataset.from_tensor_slices((trX, trY)).batch(batchsize)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_x, batch_y in train_ds:\n",
    "        batch_number += 1\n",
    "        for i_sample in range(batchsize):           \n",
    "            for k in range(K):\n",
    "                v0_state = batch_x[i_sample]\n",
    "                h0_state = hidden_layer(v0_state, W, hb)\n",
    "                v1_state = reconstructed_output(h0_state, W, vb)\n",
    "                h1_state = hidden_layer(v1_state, W, hb)\n",
    "\n",
    "                delta_W = tf.matmul(tf.transpose([v0_state]), h0_state) - tf.matmul(tf.transpose([v1_state]), h1_state)\n",
    "                W = W + alpha * delta_W\n",
    "\n",
    "                vb = vb + alpha * tf.reduce_mean(v0_state - v1_state, 0)\n",
    "                hb = hb + alpha * tf.reduce_mean(h0_state - h1_state, 0) \n",
    "\n",
    "                v0_state = v1_state\n",
    "\n",
    "            if i_sample == batchsize-1:\n",
    "                err = error(batch_x[i_sample], v1_state)\n",
    "                errors.append(err)\n",
    "                weights.append(W)\n",
    "                print ( 'Epoch: %d' % epoch, \n",
    "                       \"batch #: %i \" % batch_number, \"of %i\" % int(60e3/batchsize), \n",
    "                       \"sample #: %i\" % i_sample,\n",
    "                       'reconstruction error: %f' % err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9cbd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errors)\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9099d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W.numpy()) # a weight matrix of shape (50,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d163668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_raster_images(X=W.numpy().T, img_shape=(28, 28), tile_shape=(5, 10), tile_spacing=(1, 1))\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "image = Image.fromarray(tile_raster_images(X=W.numpy().T, img_shape=(28, 28) ,tile_shape=(5, 10), tile_spacing=(1, 1)))\n",
    "### Plot image\n",
    "plt.rcParams['figure.figsize'] = (18.0, 18.0)\n",
    "imgplot = plt.imshow(image)\n",
    "imgplot.set_cmap('gray')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52022c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.fromarray(tile_raster_images(X =W.numpy().T[10:11], img_shape=(28, 28),tile_shape=(1, 1), tile_spacing=(1, 1)))\n",
    "### Plot image\n",
    "plt.rcParams['figure.figsize'] = (4.0, 4.0)\n",
    "imgplot = plt.imshow(image)\n",
    "imgplot.set_cmap('gray')  \n",
    "plt.savefig('destructed.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e204d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('destructed.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c05d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the image to a 1d numpy array\n",
    "sample_case = np.array(img.convert('I').resize((28,28))).ravel().reshape((1, -1))/255.0\n",
    "\n",
    "sample_case = tf.cast(sample_case, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh0_p = tf.nn.sigmoid(tf.matmul(sample_case, W) + hb)\n",
    "hh0_s = tf.round(hh0_p)\n",
    "\n",
    "print(\"Probability nodes in hidden layer:\" ,hh0_p)\n",
    "print(\"activated nodes in hidden layer:\" ,hh0_s)\n",
    "\n",
    "# reconstruct\n",
    "vv1_p = tf.nn.sigmoid(tf.matmul(hh0_s, tf.transpose(W)) + vb)\n",
    "\n",
    "print(vv1_p)\n",
    "#rec_prob = sess.run(vv1_p, feed_dict={ hh0_s: hh0_s_val, W: prv_w, vb: prv_vb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535ab082",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.fromarray(tile_raster_images(X=vv1_p.numpy(), img_shape=(28, 28),tile_shape=(1, 1), tile_spacing=(1, 1)))\n",
    "plt.rcParams['figure.figsize'] = (4.0, 4.0)\n",
    "imgplot = plt.imshow(img)\n",
    "imgplot.set_cmap('gray') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56237eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
